<source>
  @type kafka_group

  brokers        ${broker_servers}
  consumer_group ${consumer_group}
  topics         ${topics} # comma-separated
  format         json
  time_source    record
  time_format    ${topics}

  # ruby-kafka consumer options
  max_bytes                ${max_bytes}
  #max_wait_time           ${max_wait_time}
  #min_bytes               ${min_bytes}
  #offset_commit_interval  ${offset_commit_interval}
  #offset_commit_threshold ${offset_commit_threshold}
  #fetcher_max_queue_size  ${fetcher_max_queue_size}
  #refresh_topic_interval  ${refresh_topic_interval}
  start_from_beginning     ${start_from_beginning}
</source>

<match **>
  @type s3

  aws_key_id  ${access_key}
  aws_sec_key ${secret_key}
  s3_bucket   ${bucket_name}
  s3_endpoint ${endpoint}
  s3_region   ${region}

  #path             logs/                # This prefix is added to each file
  force_path_style  true                 # This prevents AWS SDK from breaking endpoint URL
  time_slice_format ${time_slice_format} # This timestamp is added to each file name

  <buffer time>
    @type file

    path             ${path}
    timekey          ${timekey}          # Flush the accumulated chunks every hour
    timekey_wait     ${timekey_wait}     # Wait for 60 seconds before flushing
    timekey_use_utc  ${timekey_use_utc}  # Use this option if you prefer UTC timestamps
    chunk_limit_size ${chunk_limit_size} # The maximum size of each chunk
  </buffer>
</match>
